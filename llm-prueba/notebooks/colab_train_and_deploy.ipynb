{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c54cdc8d",
      "metadata": {},
      "source": [
        "# LLM Prueba – Colab (distilgpt2)\n",
        "\n",
        "Este notebook automatiza instalación, preparación de datos, entrenamiento (con reanudación), generación de ejemplos, subida del modelo al Hugging Face Hub y despliegue de una demo en Spaces (Gradio).\n",
        "\n",
        "Requisitos manuales:\n",
        "- Cuenta de Hugging Face y un token con permiso `write`.\n",
        "- (Opcional) Repositorio GitHub `llm-prueba` para clonar el código.\n",
        "\n",
        "Consejos de recursos (Colab Free):\n",
        "- Usa `epochs=1`, `block_size=128`, `batch_size=2`, `grad_accum_steps=8`.\n",
        "- Si se desconecta, vuelve a ejecutar y usa `--resume` para reanudar desde el último checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc7c5e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1) Configurar entorno y montar Google Drive (opcional)\n",
        "import os, sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "BASE_DIR = os.getcwd()\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount('/content/drive')\n",
        "        BASE_DIR = '/content/drive/MyDrive/llm-prueba'\n",
        "        os.makedirs(BASE_DIR, exist_ok=True)\n",
        "        %cd $BASE_DIR\n",
        "        print('Base dir:', BASE_DIR)\n",
        "    except Exception as e:\n",
        "        print('No se pudo montar Drive, se usará almacenamiento efímero del runtime.', e)\n",
        "else:\n",
        "    print('Ejecutando fuera de Colab. Base dir:', BASE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1c21c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2) Instalar dependencias y configurar cachés\n",
        "import os\n",
        "\n",
        "# Opcional: cache en Drive para datasets/modelos\n",
        "if 'BASE_DIR' in globals():\n",
        "    os.environ['HF_HOME'] = os.path.join(BASE_DIR, 'hf_cache')\n",
        "    os.environ['TRANSFORMERS_CACHE'] = os.path.join(BASE_DIR, 'hf_cache')\n",
        "    os.environ['HF_DATASETS_CACHE'] = os.path.join(BASE_DIR, 'hf_cache')\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch:', torch.__version__, 'CUDA:', torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print('Torch no disponible aún:', e)\n",
        "\n",
        "# Instalar desde requirements.txt si existe; si no, usa lista fija\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip -q install -U pip\n",
        "    !pip -q install -r requirements.txt\n",
        "else:\n",
        "    !pip -q install -U pip\n",
        "    !pip -q install transformers==4.41.2 datasets==2.19.0 accelerate==0.30.1 huggingface_hub==0.23.4 evaluate==0.4.1 gradio==4.36.1 tensorboard==2.17.0 safetensors==0.4.2 tqdm==4.66.4\n",
        "\n",
        "import torch\n",
        "print('Torch listo:', torch.__version__, 'CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0eecd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3) Verificar que el proyecto está presente en BASE_DIR\n",
        "import os, glob\n",
        "\n",
        "expected = [\n",
        "    'src/train.py',\n",
        "    'src/data/prepare_dataset.py',\n",
        "    'src/generate.py',\n",
        "]\n",
        "missing = [p for p in expected if not os.path.exists(p)]\n",
        "\n",
        "if missing:\n",
        "    print('Faltan archivos del proyecto:', missing)\n",
        "    print('\\nAcción manual necesaria:')\n",
        "    print('- Copia la carpeta completa `llm-prueba` a', os.getcwd(), 'en tu Google Drive (MyDrive).')\n",
        "    print('- Alternativa: git clone tu repo privado/público en esta carpeta y vuelve a ejecutar esta celda.')\n",
        "else:\n",
        "    print('Proyecto OK. Archivos clave presentes.')\n",
        "    print('Directorio actual:', os.getcwd())\n",
        "    print('Contenido src/:', glob.glob('src/*'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a202de33",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4) Preparar dataset (opus_books ES por defecto) { display-mode: \"form\" }\n",
        "SOURCE = \"hf_opus_books\" #@param [\"hf_opus_books\", \"local\"]\n",
        "MAX_EXAMPLES = 30000 #@param {type:\"integer\"}\n",
        "OUTPUT = \"data/dataset.txt\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "if SOURCE == 'local':\n",
        "    print('Usando archivos locales en data/raw/*.txt')\n",
        "!python -u src/data/prepare_dataset.py --source $SOURCE --max_examples $MAX_EXAMPLES --output $OUTPUT\n",
        "\n",
        "import os\n",
        "print('Tamaño dataset.txt:', os.path.getsize(OUTPUT), 'bytes')\n",
        "!python - << 'PY'\n",
        "path = 'data/dataset.txt'\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i > 10:\n",
        "            break\n",
        "        print(line.strip())\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ad9ee4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5) Entrenar modelo (con reanudación opcional) { display-mode: \"form\" }\n",
        "EPOCHS = 1 #@param {type:\"number\"}\n",
        "BLOCK_SIZE = 128 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 2 #@param {type:\"integer\"}\n",
        "GRAD_ACCUM = 8 #@param {type:\"integer\"}\n",
        "SAVE_STEPS = 200 #@param {type:\"integer\"}\n",
        "LOGGING_STEPS = 50 #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 5e-5 #@param {type:\"number\"}\n",
        "RESUME = True #@param {type:\"boolean\"}\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"-u\", \"src/train.py\",\n",
        "    \"--model_name\", \"distilgpt2\",\n",
        "    \"--dataset_path\", \"data/dataset.txt\",\n",
        "    \"--checkpoint_dir\", \"models/checkpoints\",\n",
        "    \"--final_model_dir\", \"models/final_model\",\n",
        "    \"--epochs\", str(EPOCHS),\n",
        "    \"--block_size\", str(BLOCK_SIZE),\n",
        "    \"--batch_size\", str(BATCH_SIZE),\n",
        "    \"--grad_accum_steps\", str(GRAD_ACCUM),\n",
        "    \"--learning_rate\", str(LEARNING_RATE),\n",
        "    \"--save_steps\", str(SAVE_STEPS),\n",
        "    \"--logging_steps\", str(LOGGING_STEPS),\n",
        "    \"--eval_ratio\", \"0.05\",\n",
        "]\n",
        "if RESUME:\n",
        "    cmd.append(\"--resume\")\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "subprocess.run(cmd, check=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee1bce88",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6) (Opcional) Ver métricas en TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs --port 6006"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4042bf6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 7) Generar 3 ejemplos de texto con el modelo entrenado\n",
        "!python -u src/generate.py --model_dir models/final_model --output outputs/examples.txt\n",
        "\n",
        "import os\n",
        "print('Ejemplos guardados en outputs/examples.txt, tamaño:', os.path.getsize('outputs/examples.txt'), 'bytes')\n",
        "!head -n 80 outputs/examples.txt || (Get-Content -Path outputs/examples.txt -TotalCount 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c9ff77",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 8) Configurar credenciales HF y nombres de repos { display-mode: \"form\" }\n",
        "HF_TOKEN = \"\" #@param {type:\"string\"}\n",
        "HF_USERNAME = \"\" #@param {type:\"string\"}\n",
        "MODEL_REPO_NAME = \"llm-prueba-distilgpt2-es\" #@param {type:\"string\"}\n",
        "SPACE_REPO_NAME = \"llm-prueba-demo\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "if not HF_TOKEN or not HF_USERNAME:\n",
        "    print(\"[ACCION MANUAL] Define HF_TOKEN y HF_USERNAME en esta celda y vuelve a ejecutarla. Obtén el token en https://huggingface.co/settings/tokens con permiso write.\")\n",
        "else:\n",
        "    os.environ['hf_token'] = HF_TOKEN\n",
        "    MODEL_REPO = f\"{HF_USERNAME}/{MODEL_REPO_NAME}\"\n",
        "    SPACE_REPO = f\"{HF_USERNAME}/{SPACE_REPO_NAME}\"\n",
        "    print('Repos configurados:')\n",
        "    print('MODEL_REPO =', MODEL_REPO)\n",
        "    print('SPACE_REPO =', SPACE_REPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c1957da",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 9) Subir modelo a Hugging Face Hub\n",
        "import os, sys, subprocess\n",
        "\n",
        "if 'MODEL_REPO' not in globals():\n",
        "    raise SystemExit('Define HF_TOKEN y HF_USERNAME en la celda 8 para construir MODEL_REPO y SPACE_REPO, y vuelve a ejecutar.')\n",
        "\n",
        "if not os.environ.get('HF_TOKEN'):\n",
        "    raise SystemExit('HF_TOKEN no definido. Vuelve a ejecutar la celda 8 con tu token.')\n",
        "\n",
        "if not os.path.exists('models/final_model/config.json'):\n",
        "    raise SystemExit('Modelo final no encontrado en models/final_model. Ejecuta el entrenamiento (celda 5).')\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'src/upload_to_hub.py',\n",
        "    '--model_dir', 'models/final_model',\n",
        "    '--repo_id', MODEL_REPO,\n",
        "]\n",
        "print('Subiendo a:', MODEL_REPO)\n",
        "subprocess.run(cmd, check=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a40355",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 10) Desplegar Space de Gradio en Hugging Face { display-mode: \"form\" }\n",
        "import os, sys, subprocess\n",
        "\n",
        "if 'SPACE_REPO' not in globals() or 'MODEL_REPO' not in globals():\n",
        "    raise SystemExit('Define HF_TOKEN y HF_USERNAME en la celda 8 para construir MODEL_REPO y SPACE_REPO, y vuelve a ejecutar.')\n",
        "\n",
        "if not os.environ.get('HF_TOKEN'):\n",
        "    raise SystemExit('HF_TOKEN no definido. Vuelve a ejecutar la celda 8 con tu token.')\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'deploy/deploy_space.py',\n",
        "    '--space_repo', SPACE_REPO,\n",
        "    '--model_repo', MODEL_REPO,\n",
        "]\n",
        "print('Desplegando Space:', SPACE_REPO)\n",
        "subprocess.run(cmd, check=False)\n",
        "\n",
        "print('\\nIMPORTANTE: Ve a Settings del Space y define la variable de entorno MODEL_ID con', MODEL_REPO)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab_train_and_deploy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
